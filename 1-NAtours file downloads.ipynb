{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page...\n",
      "Please solve the CAPTCHA in the browser window...\n",
      "Content successfully saved to viator_north_america.html\n"
     ]
    }
   ],
   "source": [
    "#saves the tour index for NA into file viator_north_america.html - is a bit tricky because viator hates you to do that - so you need to solve the captcha manually. Thanks Claude for solving this ;-)\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def save_webpage_content(url, output_file=\"webpage_content.html\"):\n",
    "    try:\n",
    "        # Set up Chrome options\n",
    "        chrome_options = Options()\n",
    "        # Don't run headless - we need to solve CAPTCHA\n",
    "        chrome_options.add_argument('--start-maximized')\n",
    "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        # Remove automation flags\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "        \n",
    "        # Initialize the driver\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "        \n",
    "        try:\n",
    "            # Get the page\n",
    "            print(\"Loading page...\")\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Wait for human to solve CAPTCHA\n",
    "            print(\"Please solve the CAPTCHA in the browser window...\")\n",
    "            input(\"Press Enter after solving the CAPTCHA...\")\n",
    "            \n",
    "            # Wait for main content to load\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # Get the page source after CAPTCHA\n",
    "            page_source = driver.page_source\n",
    "            \n",
    "            # Save the content to a file\n",
    "            Path(output_file).write_text(page_source, encoding='utf-8')\n",
    "            print(f\"Content successfully saved to {output_file}\")\n",
    "            \n",
    "        finally:\n",
    "            # Always close the driver\n",
    "            driver.quit()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://www.viator.com/sitemap/North-America/d8\"\n",
    "\n",
    "# Call the function\n",
    "save_webpage_content(url, \"viator_north_america.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: \"/Users/mjsteenberg/Desktop/Desktop - MJ's MacBook Air - 1/VM/viator-scraper/viator_north_america.html\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Load the local HTML file\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# html_file_path = r'C:\\Users\\mjsteenberg\\Documents\\VoiceMap\\Viator\\viator_north_america.html'  # https://www.viator.com/sitemap/North-America/d8\u001b[39;00m\n\u001b[1;32m     13\u001b[0m html_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/mjsteenberg/Desktop/Desktop - MJ\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms MacBook Air - 1/VM/viator-scraper/viator_north_america.html\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhtml_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     15\u001b[0m     content \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Parse the HTML content using BeautifulSoup\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Desktop - MJâ€™s MacBook Air - 1/VM/viator-scraper/venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: \"/Users/mjsteenberg/Desktop/Desktop - MJ's MacBook Air - 1/VM/viator-scraper/viator_north_america.html\""
     ]
    }
   ],
   "source": [
    "# uses the viator_north_america.html file to itertate through all tours and downlaod the tour details into new files in dir scrpaed_pages/NAtours\n",
    "# make sure you do not use a server that can be linked to a voicemap IP - generally speaking, you are allowed to scrape this content because it is not behind a paywall or requires user identification, but still, better not to raise any red flags\n",
    "# run the code twice, sometimes downlaods fail - on the second run, it will NOT try to downlaod already existing files\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Load the local HTML file\n",
    "# html_file_path = r'C:\\Users\\mjsteenberg\\Documents\\VoiceMap\\Viator\\viator_north_america.html'  # https://www.viator.com/sitemap/North-America/d8\n",
    "html_file_path = '/Users/mjsteenberg/Desktop/Desktop - MJ\\'s MacBook Air - 1/VM/viator-scraper/viator_north_america.html'\n",
    "with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Find the <ul> with the specific data-automation attribute\n",
    "ul_tag = soup.find('ul', {'data-automation': 'sitemap-entry-list'})\n",
    "\n",
    "# Extract the <a> tags within the <li> tags to get the URLs\n",
    "base_url = 'https://www.viator.com'\n",
    "urls = []\n",
    "for li in ul_tag.find_all('li'):\n",
    "    a_tag = li.find('a')\n",
    "    if a_tag and 'href' in a_tag.attrs:\n",
    "        # Append ?sortType=rating to each URL\n",
    "        full_url = base_url + a_tag['href'] + '?sortType=rating'\n",
    "        urls.append(full_url)\n",
    "\n",
    "# Create a directory to save the HTML files\n",
    "save_dir = '/Users/mjsteenberg/Desktop/Desktop - MJ\\'s MacBook Air - 1/VM/viator-scraper/scraped_pages/NAtours'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Function to extract the city and short part of the URL (useful for filenames)\n",
    "def extract_filename(url):\n",
    "    # Split the URL into base and query part\n",
    "    base_url_part = url.split('?')[0]\n",
    "    query_part = url.split('?')[-1]\n",
    "\n",
    "    # Extract the city (second last part) and the short URL (last part)\n",
    "    url_parts = base_url_part.split('/')\n",
    "    city_part = url_parts[-2] if len(url_parts) > 1 else ''\n",
    "    short_url_part = url_parts[-1] or url_parts[-2]\n",
    "\n",
    "    # Create a meaningful filename combining both city, short URL, and query part\n",
    "    filename = f\"{city_part}_{short_url_part}_{query_part.replace('=', '_')}.html\"\n",
    "    return filename\n",
    "\n",
    "# Loop over the URLs, scrape the content, and save it to disk\n",
    "for i, url in enumerate(urls):\n",
    "    try:\n",
    "        # Generate the filename\n",
    "        filename = extract_filename(url)\n",
    "        file_path = os.path.join(save_dir, filename)\n",
    "        \n",
    "        # Skip scraping if the file already exists\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"File already exists for {url}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Scraping URL {i+1}/{len(urls)}: {url}\")\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Referer': 'https://www.google.com/'\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Save the content to an HTML file\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(response.text)\n",
    "            print(f\"Saved {url} to {file_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve {url}, Status Code: {response.status_code}\")\n",
    "        \n",
    "        # Wait for a random time between 1 and 5 seconds before making the next request\n",
    "        sleep_time = random.uniform(1, 5)\n",
    "        print(f\"Waiting for {sleep_time:.2f} seconds before the next request...\")\n",
    "        time.sleep(sleep_time)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
